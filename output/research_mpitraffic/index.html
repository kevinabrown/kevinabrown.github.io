<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Tracking and Vizualizing MPI Link Traffic | Kevin A. Brown</title>
<link href="../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="https://kevinabrown.github.io/research_mpitraffic/">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><meta name="author" content="Kevin A. Brown">
<meta property="og:site_name" content="Kevin A. Brown">
<meta property="og:title" content="Tracking and Vizualizing MPI Link Traffic">
<meta property="og:url" content="https://kevinabrown.github.io/research_mpitraffic/">
<meta property="og:description" content="Our work illustrates the limitations of current performance analysis methods for MPI collectives on large-scale systems, and proposes a new method that employs a hardware-centric approach based on low">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2019-03-08T20:54:09-06:00">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://kevinabrown.github.io/">

                <span id="blog-title">Kevin A. Brown</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../research">Research</a>
                </li>
<li>
<a href="../blogroll">Blog</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Tracking and Vizualizing MPI Link Traffic</a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>Our work illustrates the limitations of current performance analysis methods for MPI collectives on large-scale systems, and proposes a new method that employs a hardware-centric approach based on lower-level communication metrics.</p>
<p>MPI libraries abstract the hardware layer of systems, making it hard to adequately assess the performance of message passing operations over the network layer. Tools for conducting performance analysis of MPI application try to maintain hardware portability by not penetrating this abstraction. This causes their process-centric analysis approach to fail in exposing the performance of MPI operations over the network layer.</p>
<img alt="/images/research_m_fig1.png" class="align-center" src="../images/research_m_fig1.png" style="width: 50%;"><div class="line-block">
<div class="line"><br></div>
</div>
<p>Our approach (Fig. 1) successfully exposes this performance by tracking InfiniBand network communication in Open MPI using a new trace event that we added to the Peruse interface, a performance revealing extension for MPI. This event was at a lower level, closer to the hardware layer, than all other Peruse events in Open MPI. We built a portable and non-intrusive profiling library, named ibprof, which uses this newly added Peruse event to record an application's network communication profile and write it to OTF files. The Boxfish visualization tool was used to conduct hardware-centric analysis of the profile. However, we first extended Boxfish by creating a new, generalized visualization module that is capable of representing any 2D network topology, such as the fat-tree topology used by TSUBAME2.5.</p>
<p>Our results indicated that the average communication overhead incurred by profiling was: 2.06% for an MPI_Alltoall microbenchmark, 7.63% for MPI_Bcast microbenchmark, and 0.02% for the NPB FT application benchmark. Experiments were ran on 32 nodes of the TSUBAME-KFC system. The time taken to write OTF output files by our profiler was approximately 13 milliseconds. We then showed the scalability of our method by using the Graph500 benchmark on 512 nodes of TSUBAME2.5. The benchmark’s profile was visualized in Boxfish (Fig. 2), and we showed how performance metrics from multiple domains, such as the hardware and communication domains, could be easily connected and interacted with using our network visualization.</p>
<img alt="/images/research_m_fig2.png" class="align-center" src="../images/research_m_fig2.png" style="width: 50%;"><p><strong>Figure 2: Boxfish visualization of the Graph500 benchmark</strong> <em>Our Boxfish Fat Tree module is used to visualize our profile of the Graph500 benchmark running on 512 nodes of TSUBAME2.5. Network links are shown as coloured lines connecting switches to other nodes and other switches. The colour of each link reflects how much user-data was sent over that link. Colour-value maps (one for nodes and one for links) are used to map colours to performance values.</em></p>
<div class="line-block">
<div class="line"><br></div>
</div>
<p>Finally, a case study of an under-performing MPI_Alltoallv collective proved the effectiveness of our analysis approach. In our case study, Vampir failed to provide any insight into the cause of the collective’s performance. However, for the same operation, our visualization of the network communication profile quickly identified a communication imbalance caused by the uneven distribution of data across the processes. By distributing the data more evenly, we reduced the runtime of the collective by approximately 25%.</p>
</div>
    </div>
    

</article>
</div>
        <!--End of body content-->
	
        <footer id="footer">
            Contents © 2019         <a href="mailto:kevinbrown.com@gmail.com">Kevin A. Brown</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
